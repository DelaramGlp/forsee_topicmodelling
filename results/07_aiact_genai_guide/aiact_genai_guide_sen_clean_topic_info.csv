Topic,Count,Name,Representation,Representative_Docs
-1,2,-1_backward_passes_operations_scope,"['backward', 'passes', 'operations', 'scope', 'training', 'range', 'language', 'flop', 'compute', 'models']","['example of model in scope model is trained on broad range of natural language data', 'for such models the corresponding amount of training compute measured in flop is the product of the number of full passes one full pass is the combination of forward and backward pass made during training with the total number of operations performed in full pass']"
0,334,0_models_providers_obligations_market,"['models', 'providers', 'obligations', 'market', 'downstream', 'capabilities', 'guidelines', 'compliance', 'risks', 'licence']","['downstream modifiers becoming providers of ai models with systemic risk if downstream actor modifies ai model that has been classified as ai model with systemic risk in such way that they become the provider of the modified ai model then the resulting model is presumed to have capabilities as well', 'in this case the downstream modifier must comply with the obligations for providers of ai models with systemic risk', 'in both cases the provider must comply with the obligations for providers of ai models with systemic risk from the moment the model is classified as ai model with systemic risk']"
1,55,1_compute_training_hardware_units,"['compute', 'training', 'hardware', 'units', 'gpus', 'estimation', 'models', 'capabilities', 'duration', 'thresholds']","['this list may change as technology evolves compute used to generate synthetic data which is publicly accessible since it may be indistinguishable from other publicly accessible data compute spent on purely diagnostic processes that do not contribute to enhancing model capabilities such as model evaluations or compute whose expenditure contributes to enhancing model capabilities only through lessons learnt by humans such as compute spent on exploratory research projects that result in more efficient training techniques or on experiments in synthetic data generation that fail and whose results are discarded compute used to train parent model used in distillation whether or not the parent model coincides with the concerned model compute used to train auxiliary model such as value functions or reward models whether or not the auxiliary model coincides with the concerned model recomputation of activations to save memory', 'in this regard recital ai act specifies that training of ai models takes considerable planning which includes the upfront allocation of compute resources and therefore providers of ai models are able to know if their model would meet the threshold before the training is since the planning and upfront allocation of compute resources take place before the start of the large run providers should estimate the cumulative amount of training compute that they will use before starting this run see the annexes and to these guidelines for how to estimate training compute', 'approach to estimate training compute using the approach the potential provider of ai model or ai model with systemic risk should first estimate the number of gpus or other hardware units used or expected to be used the total duration of use achieved or expected measured in seconds the peak theoretical performance of the gpus or other hardware units used or expected to be used measured in calculated via weighted average if different types of gpus or hardware units are used with different peak theoretical performances where the weights are determined by the number of hardware units of each type the average percentage of gpu utilisation achieved or expected to be achieved for the duration of use']"
2,27,2_flop_generate_trained_language,"['flop', 'generate', 'trained', 'language', 'speech', 'billion', 'examples', 'lyrics', 'models', 'data']","['while the model can generate text its training compute is not greater than flop', 'the model can generate speech and its training compute is greater than flop', 'the model can generate language and its training compute is greater than flop']"
3,20,3_tasks_range_narrow_capable,"['tasks', 'range', 'narrow', 'capable', 'performing', 'images', 'speech', 'audio', 'models', 'distinctive']","['however if the model can only competently perform narrow set of tasks transcribing speech it is not actually ai model', 'however if the model can only competently perform narrow set of tasks generating speech from text it is not actually ai model', 'training on broad range of natural language further indicates that the model should display significant generality and should be capable of competently performing wide range of distinct tasks']"
4,13,4_criterion_model_paragraph_indicative,"['criterion', 'model', 'paragraph', 'indicative', 'criteria', 'assess', 'ease', 'factors', 'availability', 'considerations']","['therefore the criterion from paragraph indicates that the model should not be ai model', 'therefore the criterion from paragraph indicates that the model should be ai model', 'therefore the criterion from paragraph indicates that the model should be ai model']"
5,12,5_yields_flop_calculations_performance,"['yields', 'flop', 'calculations', 'performance', 'peak', 'gpu', 'includes', 'approximations', 'precise', 'compute']","['the approximation from annex yields flop', 'the approximation from annex yields flop', 'the approximation from annex yields flop']"
6,11,6_tokens_transformer_architecture_language,"['tokens', 'transformer', 'architecture', 'language', 'trillion', 'decoder', 'trained', 'epochs', 'scaling', 'neural']","['model approach model is language model with transformer decoder architecture and billion parameters trained on trillion tokens', 'model approach model is language model with transformer decoder architecture and billion parameters trained on trillion tokens', 'model approach model is language model with transformer decoder architecture and billion parameters trained on trillion tokens']"
